{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "スクレイピングが完了しました\n"
     ]
    }
   ],
   "source": [
    "#ライブラリimport\n",
    "import urllib, requests, regex, csv, re, time, socks, socket, sys, math, time, datetime, random\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "\n",
    "#スクレイピング後の待機時間を定義\n",
    "scrayping_wait_time = 10\n",
    "\n",
    "#プロキシ設定\n",
    "firefox_profile = webdriver.FirefoxProfile()\n",
    "firefox_profile.set_preference('network.proxy.type', 1)\n",
    "firefox_profile.set_preference('network.proxy.socks', '127.0.0.1')\n",
    "firefox_profile.set_preference('network.proxy.socks_port', 9150)\n",
    "\n",
    "#Firefoxのパスを引数に指定\n",
    "global driver\n",
    "driver = webdriver.Firefox(firefox_profile=firefox_profile)\n",
    "\n",
    "#確認くん\n",
    "driver.get(\"https://www.ugtop.com/spill.shtml\")\n",
    "time.sleep(scrayping_wait_time)\n",
    "\n",
    "#################################\n",
    "#格納用CSVを生成\n",
    "#################################\n",
    "\n",
    "#CSVファイルを読み込み\n",
    "input_file = \"Indeed_Scrayping_\" + str(datetime.date.today()) + \".csv\"\n",
    "\n",
    "#CSVがなければ、ヘッダーを作成\n",
    "try:\n",
    "    input_df = pd.read_csv(input_file,encoding='cp932')\n",
    "except:\n",
    "    #各項目を格納する用の配列\n",
    "    job_detail_url_list = []\n",
    "    company_name_list = []\n",
    "    work_location_list = []\n",
    "    empstatus_list = []\n",
    "    salary_list = []\n",
    "    \n",
    "    #各項目のヘッダーを格納\n",
    "    job_detail_url_list.append(\"求人詳細URL\")\n",
    "    company_name_list.append(\"企業名\")\n",
    "    work_location_list.append(\"勤務地\")\n",
    "    empstatus_list.append(\"雇用形態\")\n",
    "    salary_list.append(\"給与\")\n",
    "\n",
    "    #CSVに出力する\n",
    "    data = zip(job_detail_url_list,company_name_list,work_location_list,empstatus_list,salary_list)\n",
    "    with open(input_file,'a',errors='backslashreplace',encoding=\"SHIFT-JIS\") as fout:\n",
    "        writecsv = csv.writer(fout,lineterminator='\\n')\n",
    "        writecsv.writerows(data)\n",
    "\n",
    "#キーワードのエンコード\n",
    "keyword = \"エンジニア\"\n",
    "keywords = urllib.parse.quote(keyword)\n",
    "\n",
    "time.sleep(scrayping_wait_time)\n",
    "\n",
    "#ページのパラメータの初期値\n",
    "page_parameter = 0\n",
    "#ページのパラメータの上限を設定（Max100ページ）\n",
    "page_parameter_max= 990\n",
    "\n",
    "#Maxの100ページまで繰り返し\n",
    "for i in range(page_parameter_max):\n",
    "    \n",
    "    #詳細URLリスト\n",
    "    job_detail_url_list = []\n",
    "    #会社名リスト\n",
    "    company_name_list = []\n",
    "    #勤務地リスト\n",
    "    work_location_list = []\n",
    "    #雇用形態リスト\n",
    "    empstatus_list = []\n",
    "    #給与リスト\n",
    "    salary_list = []\n",
    "    \n",
    "    #求人一覧ページ\n",
    "    job_list_page_url = \"https://jp.indeed.com/jobs?q=\" + keywords + \"&start=\" + str(page_parameter)\n",
    "\n",
    "    #指定したURLに遷移\n",
    "    driver.get(job_list_page_url)\n",
    "    time.sleep(scrayping_wait_time)\n",
    "\n",
    "    #Beautifulsoupで求人一覧URLを取得\n",
    "    html = driver.page_source\n",
    "    soup = BeautifulSoup(html, 'lxml')\n",
    "    time.sleep(scrayping_wait_time)\n",
    "\n",
    "    for maintable in soup.findAll(class_=\"jobsearch-SerpJobCard unifiedRow row result clickcard\"):\n",
    "        \n",
    "        #「詳細URL」を取得して格納\n",
    "        try:\n",
    "            table = maintable.find(\"h2\")\n",
    "            parts = table.find(\"a\")\n",
    "            job_detail_url_list.append(\"https://jp.indeed.com\" + parts.get(\"href\"))\n",
    "        except AttributeError:\n",
    "            job_detail_url_list.append(\"情報なし\")\n",
    "\n",
    "        #「会社名」を取得して格納\n",
    "        try:\n",
    "            table = maintable.find(class_=\"company\")\n",
    "            company_name = re.sub(\"\\r|\\t|\\n|\\u3000| \",\"\",table.text)\n",
    "            company_name_list.append(company_name)\n",
    "        except AttributeError:\n",
    "            company_name_list.append(\"情報なし\")\n",
    "            \n",
    "        #「勤務地」を取得して格納\n",
    "        try:\n",
    "            table = maintable.find(class_=\"location accessible-contrast-color-location\")\n",
    "            work_location = re.sub(\"\\r|\\t|\\n|\\u3000| \",\"\",table.text)\n",
    "            work_location_list.append(work_location)\n",
    "        except AttributeError:\n",
    "            work_location_list.append(\"情報なし\")\n",
    "\n",
    "        #「雇用形態」を取得して格納\n",
    "        try:\n",
    "            table = maintable.find(class_=\"jobTypeLabelsWrapper\")\n",
    "            empstatus = re.sub(\"\\r|\\t|\\n|\\u3000| \",\"\",table.text)\n",
    "            empstatus_list.append(empstatus)\n",
    "        except AttributeError:\n",
    "            empstatus_list.append(\"情報なし\")\n",
    "\n",
    "        #「給与」を取得して格納\n",
    "        try:\n",
    "            table = maintable.find(class_=\"salaryText\")\n",
    "            salary = re.sub(\"\\r|\\t|\\n|\\u3000| \",\"\",table.text)\n",
    "            salary_list.append(salary)\n",
    "        except AttributeError:\n",
    "            salary_list.append(\"情報なし\")\n",
    "\n",
    "    #CSVに出力する　※一旦、csvの保存先はこのファイルと同階層で設定。後で../99_Outputへつなぐ\n",
    "    data = zip(job_detail_url_list,company_name_list,work_location_list,empstatus_list,salary_list)\n",
    "    with open(\"Indeed_Scrayping_\" + str(datetime.date.today()) + \".csv\",'a',errors='backslashreplace',encoding=\"SHIFT-JIS\") as fout:\n",
    "        writecsv = csv.writer(fout,lineterminator='\\n')\n",
    "        writecsv.writerows(data)\n",
    "    \n",
    "    #ページのパラメータを10増やす\n",
    "    page_parameter += 10\n",
    "\n",
    "    print(\"{}ページ目が完了しました。\".format(i+1))\n",
    "    \n",
    "    #100件目以降は表示されないため、breakしてブラウザを閉じる\n",
    "    if page_parameter >= 10:\n",
    "        break\n",
    "\n",
    "print(\"スクレイピングが完了しました\")\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
